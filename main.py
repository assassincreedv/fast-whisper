from faster_whisper import WhisperModel,BatchedInferencePipeline
import sys
import os
import time

def transcribe(audio_path):
    if not os.path.isfile(audio_path):
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {audio_path}")
        return

    total_start = time.time()

    # Step 1: åŠ è½½æ¨¡å‹
    model_start = time.time()
    model = WhisperModel("distil-large-v3", device="cuda", compute_type="float16")

    batched_model = BatchedInferencePipeline(model=model)
    model_end = time.time()
    print(f"âœ… æ¨¡å‹åŠ è½½è€—æ—¶: {model_end - model_start:.2f} ç§’")

    # Step 2: å¼€å§‹è½¬å½•
    transcribe_start = time.time()
    segments, info = batched_model.transcribe(audio=audio_path, beam_size=5, batch_size=16, word_timestamps=True)
    transcribe_end = time.time()
    print(f"ğŸ•’ éŸ³é¢‘è½¬å½•è€—æ—¶: {transcribe_end - transcribe_start:.2f} ç§’")

    # Step 3: è¾“å‡ºç»“æœ
    print(f"\nğŸ” æ£€æµ‹è¯­è¨€: {info.language}")
    print("\nğŸ“‹ è½¬å½•å†…å®¹ï¼š\n")
    for segment in segments:
        print(f"[{segment.start:.2f}s - {segment.end:.2f}s] {segment.text}")

    total_end = time.time()
    print(f"\nğŸš€ æ€»è€—æ—¶: {total_end - total_start:.2f} ç§’")

if __name__ == "__main__":
    if len(sys.argv) < 2:
        print("ç”¨æ³•: python3 transcribe_audio.py <éŸ³é¢‘è·¯å¾„>")
    else:
        transcribe(sys.argv[1])
